{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    预处理层\n",
    "    将图像进行标准化处理\n",
    "    \"\"\"\n",
    "    mean = [0.485, 0.456, 0.406] \n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "    preprocessed_img = img.copy()[:, :, ::-1] # BGR > RGB\n",
    "    \n",
    "    #标准化处理， 将bgr三层都处理\n",
    "    for i in range(3):\n",
    "\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - mean[i]\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
    "        \n",
    "    preprocessed_img = \\\n",
    "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1))) #transpose HWC > CHW\n",
    "    preprocessed_img = torch.from_numpy(preprocessed_img) #totensor\n",
    "    preprocessed_img.unsqueeze_(0)\n",
    "    input = torch.tensor(preprocessed_img, requires_grad=True)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET) #利用色彩空间转换将heatmap凸显\n",
    "    heatmap = np.float32(heatmap)/255 #归一化\n",
    "    cam = heatmap + np.float32(img) #将heatmap 叠加到原图\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite('GradCam_test.jpg', np.uint8(255 * cam))#生成图像\n",
    "    \n",
    "    cam = cam[:, :, ::-1] #BGR > RGB\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np.uint8(255*cam))\n",
    "    \n",
    "class FeatureExtractor():\n",
    "    \"\"\"\n",
    "    1. 提取目标层特征\n",
    "    2. register 目标层梯度\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.model_features = model.features\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = list()\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n",
    "    def __call__(self, x):\n",
    "        target_activations = list()\n",
    "        self.gradients = list()\n",
    "        for name, module in self.model_features._modules.items(): #遍历的方式遍历网络的每一层\n",
    "            x = module(x) #input 会经过遍历的每一层\n",
    "            if name in self.target_layers: #设个条件，如果到了你指定的层， 则继续\n",
    "                x.register_hook(self.save_gradient) #利用hook来记录目标层的梯度\n",
    "                target_activations += [x] #这里只取得目标层的features\n",
    "        x = x.view(x.size(0), -1) #reshape成 全连接进入分类器\n",
    "        x = self.model.classifier(x)#进入分类器\n",
    "        return target_activations, x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam():\n",
    "    \"\"\"\n",
    "    GradCam主要执行\n",
    "    1.提取特征（调用FeatureExtractor)\n",
    "    2.反向传播求目标层梯度\n",
    "    3.实现目标层的CAM图\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer_names):\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "        self.extractor = FeatureExtractor(self.model, target_layer_names)\n",
    "        # 此类返回两个参数，一个是目标层的feature map构成的list，一个是分类器的输出\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    def __call__(self, input):\n",
    "        features, output = self.extractor(input) #这里的feature 对应的就是目标层的输出， output是图像经过分类网络的输出\n",
    "        # print(features[0].shape,type(features[0]), output.shape)\n",
    "        '''\n",
    "        features:list len=1, feature[0]-->shape: (1, 512, 14, 14) type:torch.tensor\n",
    "        output: shape--> [1, 1000] type:torch.tensor\n",
    "        '''\n",
    "        output.data\n",
    "        one_hot = output.max() #取1000个类中最大的值,就是对应分类种类的神经元\n",
    "        # print(one_hot)\n",
    "            \n",
    "        self.model.features.zero_grad() #梯度清零\n",
    "        self.model.classifier.zero_grad() #梯度清零\n",
    "        one_hot.backward(retain_graph=True) #反向传播之后，为了取得目标层梯度\n",
    "        \n",
    "        grad_val = self.extractor.get_gradients()[-1].data #.numpy()\n",
    "        #调用函数get_gradients(),  得到目标层求得的梯度\n",
    "        #print(\"grad_val'shape :{}\".format(grad_val.shape))\n",
    "        #print(\"grad_val'type :{}\".format(type(grad_val))\n",
    "              \n",
    "        '''\n",
    "        grad_val shape:[1, 512, 14, 14] type:torch.tensor\n",
    "        '''\n",
    "              \n",
    "        target = features[-1] \n",
    "              \n",
    "        #print(\"target :{}\".format(target.shape))\n",
    "        #print(\"target's type :{}\".format(type(target)))\n",
    "        #features 目前是list 要把里面relu层的输出取出来, 也就是我们要的目标层 shape(1, 512, 14, 14)\n",
    "              \n",
    "        target = target.data.squeeze(0) #(1, 512, 14, 14) > (512, 14, 14) \n",
    "              \n",
    "        #print(\"target :{}\".format(target.shape))\n",
    "        #print(\"target's type :{}\".format(type(target)))\n",
    "        \n",
    "        weights = F.adaptive_avg_pool2d(target, 1)[0, :] #array shape (512, 1, 1) 求出relu梯度的 512层 每层权重\n",
    "              \n",
    "        print(\"weights shape:{}\".format(weights.shape))\n",
    "              \n",
    "        cam = torch.zeros(target.shape[1:])\n",
    "              \n",
    "        #print(cam.shape)\n",
    "        # cam = np.zeros(target.shape[1:]) #做一个空白map，待会将值填上\n",
    "        #(14, 14)  shape(512, 14, 14)tuple  索引[1:] 也就是从14开始开始\n",
    "        \n",
    "        #for loop的方式将平均后的权重乘上目标层的每个feature map， 并且加到刚刚生成的空白map上\n",
    "        for i, w in enumerate(weights): \n",
    "            cam += w * target[i, :, :] \n",
    "            #w * target[i, :, :]\n",
    "            #target[i, :, :] = array:shape(14, 14)\n",
    "            #w = 512个的权重均值 shape(512, )\n",
    "            #每个均值分别乘上target的feature map\n",
    "            #在放到空白的14*14上（cam)\n",
    "            #最终 14*14的空白map 会被填满\n",
    "        print(\"cam shape:{}\".format(cam.shape))\n",
    "        cam = torch.reshape(cam, (224, 224)) #将14*14的featuremap 放大回224*224\n",
    "        cam = cam - torch.min(cam)\n",
    "        cam = cam  / torch.max(cam)\n",
    "        cam = cam.numpy()\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam = GradCam(model = models.vgg16(pretrained=True), \\\n",
    "    target_layer_names = [\"29\"])\n",
    "\n",
    "#使用预训练vgg16\n",
    "#我们的目标层取第29层relu, relu层只保留有用的结果， 所以取其层最能突显出特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape:torch.Size([1, 1])\n",
      "cam shape:torch.Size([14, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[224, 224]' is invalid for input of size 196",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-fc6626c00d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mshow_cam_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-443b0e43b935>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#最终 14*14的空白map 会被填满\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cam shape:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#将14*14的featuremap 放大回224*224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m  \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[224, 224]' is invalid for input of size 196"
     ]
    }
   ],
   "source": [
    "\n",
    "img = cv2.imread('./images/crocodiles.jpg') #读取图像\n",
    "img = np.float32(cv2.resize(img, (224, 224))) / 255 #为了丢到vgg16要求的224*224 先进行缩放并且归一化\n",
    "input = preprocess_image(img)\n",
    "\n",
    "mask = grad_cam(input)\n",
    "show_cam_on_image(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
